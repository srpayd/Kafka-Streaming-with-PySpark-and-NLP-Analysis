{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming with Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS']= '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:2.4.3 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies\n",
    "- We need to import the necessary pySpark modules for Spark, Spark Streaming, and Spark Streaming with Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder \\\n",
    "                  .master(\"local[4]\") \\\n",
    "                  .appName(\"KafkaWordCount\") \\\n",
    "                  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Streaming Context\n",
    "- We pass the Spark context along with the batch duration which here is set to 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "myschema = StructType([StructField(c, StringType()) for c in [\"timestamp\", \"info\", \"country\", \"message\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Kafka\n",
    "- The topic connected to is **test**, from consumer group spark-streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\",\"localhost:9092\") \\\n",
    "        .option(\"subscribe\",\"deneme2\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message Processing\n",
    "- Structure Streaming API (dataframe) provides functions to count the number of messages in the log, and to print them to the output:\n",
    "\n",
    "**timestamp** <br/> **country** <br/> **count**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawQuery = df \\\n",
    "            .writeStream \\\n",
    "            .queryName(\"qraw\")\\\n",
    "            .format(\"memory\")\\\n",
    "            .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "|key|value|topic|partition|offset|timestamp|timestampType|\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw = spark.sql(\"select * from qraw\")\n",
    "raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the \"value\" from Kafka message\n",
    "df2=df.selectExpr(\"CAST(value AS STRING)\", \"CAST(timestamp AS TIMESTAMP)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split messages by space and get a column\n",
    "split_col = F.split(df2['value'], ' ')\n",
    "df3 = df2.withColumn('timestamp', split_col.getItem(0))\n",
    "df3 = df3.withColumn('info', split_col.getItem(1))\n",
    "df3 = df3.withColumn('country', split_col.getItem(2))\n",
    "df3 = df3.withColumn('message', split_col.getItem(3)).drop('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- info: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display d3 dataframe's schema\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of country per timestamp\n",
    "word_counts=df3.groupBy(F.col(\"timestamp\"),F.col(\"country\")) \\\n",
    "               .count() \\\n",
    "               .sort(desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Due to will be recording the output of word_counts into Casandra, we need to create a table in Casandra that has same schema of word_counts dataframe.\n",
    "word_counts.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Casandra. \n",
    "# query = word_counts.writeStream \\\n",
    "#                    .option(\"checkpointLocation\", '/tmp/check_point/') \\\n",
    "#                    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "#                    .option(\"keyspace\", \"demo\") \\\n",
    "#                    .option(\"table\", \"test\") \\\n",
    "#                    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Jupyter console.\n",
    "query = word_counts.writeStream \\\n",
    "                   .outputMode(\"complete\") \\\n",
    "                   .format(\"console\") \\\n",
    "                   .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
